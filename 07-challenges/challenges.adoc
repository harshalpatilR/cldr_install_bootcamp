= Challenges - FCE New Hire Boot Camp

August 16, 2019

* Overview
** Build a CM-managed CDH cluster and test it
* Place your work in the `07-challenges/labs` folder
** All text files require AsciiDoc (`.adoc`) extension and formatting
** All screenshots must be in PNG format
** You will create each required file yourself
** Please document you process carefully. We can only mark what we understand.
** **Please follow instructions CAREFULLY**
* You may consult with each other and research online
** Submit only your own work
* Push to GitHub often -- don't wait until the end
* If you break your cluster, or your cluster breaks you:
** Push the work you have into GitHub
** Create an Issue to describe what you think went wrong
** Notify the instructor

== Challenge Setup

* Create the Issue `Challenges Setup`
* Make sure `araujo`, `alexciobanu` are Collaborators
* Make sure your GitHub labels have been configured
* Assign the Issue to yourself and label it `started`
* Add the following Linux accounts to **all** nodes
** User `nicholas` with a UID of `1979`
** User `jesse` with a UID of `1999`
** Create the group `herbal` and add `nicholas` to it
** Create the group `life` and add `jesse` to it
* In the file `07-challenges/labs/0_setup.adoc`:
** List the cloud provider you are using (AWS, GCE, Azure, CloudCat, other)
** List your instances by IP address and DNS name (don't use `/etc/hosts` for this)
** List the Linux release you are using
** List the file system capacity for the first node
** List the command and output for `yum repolist enabled`
** List the `/etc/passwd` entries for `nicholas` and `jesse`
*** Do not list the entire file
** List the `/etc/group` entries for `herbal` and `life`
*** Do not list the entire file
** List the output of the following commands:
*** `getent group herbal`
*** `getent passwd life`
* Push these updates to GitHub
* Label your Issue `review`
* Assign the Issue to an instructor

== Challenge 1: Install a MariaDB server

* Create the Issue `Install MariaDB` 
* Assign the Issue to yourself and label it `started`
* Install MariaDB *5.5* server on the second node listed in `0_setup.adoc`
** Use a YUM repository to install the package
** Copy the repo configuration you used to `07-challenges/labs/1_my-database-server.repo.adoc`
* On all cluster nodes
** Install the database client package and JDBC connector jar on all nodes
* Start the database service and create these databases
** `scm`
** `rman`
** `hive`
** `oozie`
** `nav`
** `hue`
* Record the following in `07-challenges/labs/1_db-server.adoc`
** A command and output that shows the hostname of your database server
** A command and output that reports the database server version
** A command and output that lists all the databases in the server
* Push this work to GitHub
* Label the Issue `review` and assign it to an instructor

== Challenge 2: Install Cloudera Manager

* Create the Issue `Install CM`
* Assign yourself and label it `started`
* Install Cloudera Manager **6.2.0** on the last node listed in `0_setup.adoc`
* List the command and output for `ls /etc/yum.repos.d` in `07-challenges/labs/2_cm.adoc`
** Copy `cloudera-manager.repo` to `07-challenges/labs/2_cloudera-manager.repo.adoc`
* Connect Cloudera Manager Server to its database
** Use the `scm_prepare_database.sh` script to create the `db.properties` file
*** List the full command and its output in `2_cm.adoc`
* Start the Cloudera Manager server
* In `07-challenges/labs/2_db.properties.adoc` add:
** The first line of the server log
** The line(s) that contain the phrase "Started Jetty server"
** The content of the `db.properties` file
* Push these changes to GitHub and label the Issue `review`
* Assign the issue to an instructor

== Challenge 3 - Install the lastest **CDH 6.1.0** version

* Create the Issue `Install CDH`
* Assign yourself and label it `started`
* **READ THIS**: Note that you're **NOT** installing CDH 6.2.x
* Deploy Core set services + Kudu
** Rename your cluster after your GitHub handle
* Create user directories in HDFS for `nicholas` and `jesse`
** Ensure the owner and group for each directory is the corresponding user and group
* Add the following to `3_cm.adoc`:
** The command and output for `hdfs dfs -ls /user`
** The command and output from the CM API call `../api/v14/hosts`
** The command and output from the CM API call `../api/v8/clusters/<githubName>/services`
* Login to Hue and install the Hive sample data
** Use `beeline` to display the `default` database tables
** Copy the output to `07-challenges/labs/3_beeline.png`

* Push this work to GitHub and label the Issue `review`
* Assign the issue to an instructor

== Challenge 4 - HDFS Testing

* Create the Issue `Test HDFS`
* Assign yourself and label it `started`
* As user `nicholas`, use `teragen` to generate a 6,6200,000-record dataset
** Write the output to 4 files
** Set the block size to 32 MB
** Set the mapper container size to 768 MiB
** Name the target directory `tgen`
** Use the `time` command to capture job duration
* Put the following in `07-challenges/labs/4_teragen.adoc`
** The full `teragen` command and output
** The result of the `time` command
** The command and output of `hdfs dfs -ls /user/nicholas/tgen`
** The command and output of `hadoop fsck -blocks /user/nicholas`
* Push this work to GitHub and label the Issue `review`
* Assign the issue to an instructor

== Challenge 5 - Tune the cluster

* Create the Issue `Tune cluster`
* Assign the issue to yourself and label it `started`
* Change the memory allocation for YARN to maximally use the resources in the cluster for both Spark and MapReduce jobs

* Run the `terasort` program as user `jesse` with the output target `/user/jesse/tsort`
** Use the `time` command to prove that the number of mappers chosen produces the ideal time for executing teragen.
** Copy the execution times and tuning parameters changed into `07-challenges/labs/5_terasort.adoc`
* Push this work to GitHub and label the Issue `review`
* Assign the issue to the instructor

== Challenge 6 - Install & Configure the Sentry Service

* Create the Issue `Install Sentry`
** Label it `started`
* Use Cloudera Manager to install and enable Sentry
** Note for this lab you do not need to enable kerberos
* Configure both Hive & Impala to use Sentry
* Create a role for `HttpViewer` that can read the `web_logs` database
** Assign the `herbal` group to this role
* Create a role for `ServiceViewer` that can read the `customers` databases
** Assign the `life` group to this role
* Use `beeline` to select ten records from `web_logs`
* Use `beeswax` to select ten records from `customers`
* Capture each outcome as a screenshot, `6_beeline.png` and `6_beeswax.png`
* Label the issue `review`
* Assign the issue to the instructor
* Push all work to GitHub

== When time runs out:

* Commit any outstanding changes from your repo to GitHub
* Notify `araujo@cloudera.com` and `alex.ciobanu@cloudera.com` once you have stopped pushing to your repo
